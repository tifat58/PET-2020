{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_inversion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPzbXZLAVyt4vP8jwZd1Hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tifat58/PET-2020/blob/master/model_inversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqXCFobXn8Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Subset, Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9pWJZ-lqzvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" LeNet architecture implementation\n",
        "\"\"\"\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
        "        self.fc1   = nn.Linear(16*5*5, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KdZXo0Yq4mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_inits(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "    \n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiRUzYcGq7_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, data_loader, scheduler, num_epochs=20):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "                \n",
        "                for data in data_loader:\n",
        "                    inputs, labels = data\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    with torch.set_grad_enabled(True):\n",
        "                        outputs = model(inputs)\n",
        "                        \n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "#                         print(preds, labels)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                scheduler.step()\n",
        "                \n",
        "                epoch_loss = running_loss / train_size\n",
        "                epoch_acc = running_corrects.double() / train_size\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format('Train', epoch_loss, epoch_acc))\n",
        "                    \n",
        "    #         else:\n",
        "                \n",
        "    #             model.eval()   # Set model to evaluate mode\n",
        "                \n",
        "    #             running_loss = 0.0\n",
        "    #             running_corrects = 0\n",
        "                \n",
        "    #             for data in test_loader:\n",
        "    #                 inputs, labels, idx = data\n",
        "    #                 inputs = inputs.to(device)\n",
        "    #                 labels = labels.to(device)\n",
        "    #                 optimizer.zero_grad()\n",
        "    #                 with torch.set_grad_enabled(False):\n",
        "    #                     outputs = model(inputs)\n",
        "    #                     _, preds = torch.max(outputs, 1)\n",
        "                        \n",
        "    #                     loss = criterion(outputs, labels)\n",
        "                \n",
        "    #                 running_loss += loss.item() * inputs.size(0)\n",
        "    #                 running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "    #             epoch_loss = running_loss / test_size\n",
        "    #             epoch_acc = running_corrects.double() / test_size\n",
        "                \n",
        "    #             print('{} Loss: {:.4f} Acc: {:.4f}'.format('Val', epoch_loss, epoch_acc))\n",
        "    #             if epoch_acc > best_acc:\n",
        "    #                 best_acc = epoch_acc\n",
        "    #                 best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    #     print()\n",
        "\n",
        "    # time_elapsed = time.time() - since\n",
        "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    # print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # # load best model weights\n",
        "    # model.load_state_dict(best_model_wts)\n",
        "    # return model, best_acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z50BY9Brb6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, criterion, optimizer, data_loader):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "                \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / test_size\n",
        "    epoch_acc = running_corrects.double() / test_size\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('Val', epoch_loss, epoch_acc))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTLxvFevrmP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    #  transforms.Normalize((0.5), (0.5))\n",
        "     ])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZmub7o4uIrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "2ff1a741-6672-4bf8-a06d-6fafbe464ef2"
      },
      "source": [
        "train_size = len(train_dataset)\n",
        "test_size = len(test_dataset)\n",
        "# net = torch.load('LeNet_5_class_pretrained_model.tar')\n",
        "net = LeNet()\n",
        "# net.apply(weight_inits)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "train_model(net, criterion, optimizer_ft, trainloader, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "Train Loss: 0.2830 Acc: 0.9121\n",
            "Epoch 1/9\n",
            "----------\n",
            "Train Loss: 0.0809 Acc: 0.9746\n",
            "Epoch 2/9\n",
            "----------\n",
            "Train Loss: 0.0565 Acc: 0.9821\n",
            "Epoch 3/9\n",
            "----------\n",
            "Train Loss: 0.0458 Acc: 0.9856\n",
            "Epoch 4/9\n",
            "----------\n",
            "Train Loss: 0.0387 Acc: 0.9881\n",
            "Epoch 5/9\n",
            "----------\n",
            "Train Loss: 0.0311 Acc: 0.9904\n",
            "Epoch 6/9\n",
            "----------\n",
            "Train Loss: 0.0266 Acc: 0.9909\n",
            "Epoch 7/9\n",
            "----------\n",
            "Train Loss: 0.0122 Acc: 0.9964\n",
            "Epoch 8/9\n",
            "----------\n",
            "Train Loss: 0.0097 Acc: 0.9972\n",
            "Epoch 9/9\n",
            "----------\n",
            "Train Loss: 0.0081 Acc: 0.9978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG4bqDX0xDKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "801faa59-ed2d-49de-d8ad-1743d7ec816b"
      },
      "source": [
        "test_model(net, criterion, optimizer_ft, testloader)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 0.0256 Acc: 0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htc2LyWNxcUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_train(x, y, optimizer, model, epoch=1000):\n",
        "    model.eval()\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        \n",
        "        prob = torch.softmax(logits, -1)\n",
        "        loss = y * prob.log()\n",
        "        loss = - loss.sum(-1).mean()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 100 == 0:\n",
        "            print('loss', loss.item())\n",
        "            \n",
        "    print(\"Image training finished...\")\n",
        "    x = torch.tanh(x)\n",
        "    return x,y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqCAKhdoyKWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "01bfcd37-6c76-4ee3-f816-4c57de7c4a34"
      },
      "source": [
        "y = torch.FloatTensor([0.0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "x = torch.randn((1, 1, 28, 28), requires_grad=True)\n",
        "optimizer = optim.Adam([x], lr=0.001)\n",
        "images, labels = image_train(x, y, optimizer, net, 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 4.46278715133667\n",
            "loss 0.18367625772953033\n",
            "loss 0.0674540176987648\n",
            "loss 0.03492016717791557\n",
            "loss 0.022810442373156548\n",
            "loss 0.015010515227913857\n",
            "loss 0.009723697789013386\n",
            "loss 0.0069838883355259895\n",
            "loss 0.005387906916439533\n",
            "loss 0.004345367196947336\n",
            "Image training finished...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax79LMZ1z4f8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "9bee5f84-3dcb-4352-9d95-eb911cca710c"
      },
      "source": [
        "pic = images.detach().numpy()\n",
        "print(pic.shape)\n",
        "pic_1 = pic[0,0,:,:]\n",
        "plt.imshow(pic_1, cmap='Greys', interpolation='nearest')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f54241e74a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGroUbyq1LvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4a6c4760-5a34-4234-dfac-999fe4727793"
      },
      "source": [
        "y_pred = net(images)\n",
        "y_pred"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9.4282,  2.0327, -1.5417, -1.3636, -1.0980, -0.5980, -4.9631, -2.4809,\n",
              "          0.8561, -0.2797]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}